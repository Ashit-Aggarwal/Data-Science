{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gesture Recognition\n\nA home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n\nThe gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command that are as followed:\n\nGesture | Action\n--------|--------\nThumbs up |  Increase the volume\nThumbs down | Decrease the volume\nLeft swipe | 'Jump' backwards 10 seconds\nRight swipe | 'Jump' forward 10 seconds  \nStop | Pause the movie\n<br>\n<br>\n\n### Objectives:<br>\n1. **Generator:** The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully. <br>\n\n2. **Model:** Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further. <br>\n\n3. **Write up:** This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model.<br>","metadata":{}},{"cell_type":"markdown","source":"### Importing Packages and setting up seed value","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport datetime\nimport os\nimport matplotlib.pyplot as plt\nfrom imageio import imread\nfrom skimage import io, transform\nfrom skimage.transform import resize\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"OcPsZCLlR_G4","execution":{"iopub.status.busy":"2022-04-12T12:42:51.740488Z","iopub.execute_input":"2022-04-12T12:42:51.740946Z","iopub.status.idle":"2022-04-12T12:42:53.032576Z","shell.execute_reply.started":"2022-04-12T12:42:51.740858Z","shell.execute_reply":"2022-04-12T12:42:53.031808Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)","metadata":{"id":"WRRzV3bgSN0l","execution":{"iopub.status.busy":"2022-04-12T12:42:53.034153Z","iopub.execute_input":"2022-04-12T12:42:53.034426Z","iopub.status.idle":"2022-04-12T12:42:57.850153Z","shell.execute_reply.started":"2022-04-12T12:42:53.034390Z","shell.execute_reply":"2022-04-12T12:42:57.849412Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize path and vairables","metadata":{}},{"cell_type":"code","source":"train_doc = np.random.permutation(open('../input/gesture-recognition/train.csv').readlines())\nval_doc = np.random.permutation(open('../input/gesture-recognition/val.csv').readlines())\n\n# Batch size handled in class\n\nbatch_size = 30\nimg_size_ht = 120\nimg_size_wd = 120","metadata":{"id":"dF8BzXNCSOlU","execution":{"iopub.status.busy":"2022-04-12T12:42:57.851437Z","iopub.execute_input":"2022-04-12T12:42:57.851670Z","iopub.status.idle":"2022-04-12T12:42:57.884298Z","shell.execute_reply.started":"2022-04-12T12:42:57.851639Z","shell.execute_reply":"2022-04-12T12:42:57.883688Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Defining Generator Function","metadata":{}},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size, factor_ht=0, factor_wd=0 ):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    \n    img_idx = np.arange(0,30,2) #create a list of image numbers you want to use for a particular video\n    \n    while True:\n        t = np.random.permutation(folder_list)\n\n\n        num_batches = len(folder_list)//batch_size # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                   \n                    # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n        rem_image = len(folder_list)%batch_size\n        batch += 1\n        if(rem_image!=0):\n            batch_data = np.zeros((rem_image,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n            for folder in range(rem_image): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    \n                   # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:42:57.887381Z","iopub.execute_input":"2022-04-12T12:42:57.887574Z","iopub.status.idle":"2022-04-12T12:42:57.911102Z","shell.execute_reply.started":"2022-04-12T12:42:57.887550Z","shell.execute_reply":"2022-04-12T12:42:57.910392Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def generator_new(source_path, folder_list, batch_size, factor_ht=0, factor_wd=0 ):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    \n    img_idx = np.arange(0,30,2) #create a list of image numbers you want to use for a particular video\n    \n    while True:\n        t = np.random.permutation(folder_list)\n\n\n        num_batches = len(folder_list)//batch_size # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                   \n                    # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n#                     image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n        rem_image = len(folder_list)%batch_size\n        batch += 1\n        if(rem_image!=0):\n            batch_data = np.zeros((rem_image,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n            for folder in range(rem_image): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    \n                   # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n#                     image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:42:57.913275Z","iopub.execute_input":"2022-04-12T12:42:57.913844Z","iopub.status.idle":"2022-04-12T12:42:57.934374Z","shell.execute_reply.started":"2022-04-12T12:42:57.913800Z","shell.execute_reply":"2022-04-12T12:42:57.933581Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size, factor_ht=0, factor_wd=0 ):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    \n    img_idx = np.arange(0,30,2) #create a list of image numbers you want to use for a particular video\n    \n    while True:\n        t = np.random.permutation(folder_list)\n\n\n        num_batches = len(folder_list)//batch_size # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                   \n                    # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n        rem_image = len(folder_list)%batch_size\n        batch += 1\n        if(rem_image!=0):\n            batch_data = np.zeros((rem_image,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n            for folder in range(rem_image): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    \n                   # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels\n            ","metadata":{"id":"l9HmSX-ZSPYA","execution":{"iopub.status.busy":"2022-04-12T12:42:57.935754Z","iopub.execute_input":"2022-04-12T12:42:57.936074Z","iopub.status.idle":"2022-04-12T12:42:57.959203Z","shell.execute_reply.started":"2022-04-12T12:42:57.936041Z","shell.execute_reply":"2022-04-12T12:42:57.958260Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Fit function for the models","metadata":{}},{"cell_type":"code","source":"def run_model(model, batch_size = 30, epoch_num=25):\n        \n        # Let us create the train_generator and the val_generator which will be used in .fit_generator.\n        train_generator = generator(train_path, train_doc, batch_size)\n        val_generator = generator(val_path, val_doc, batch_size)\n        \n\n        model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \n        if not os.path.exists(model_name):\n            os.mkdir(model_name)\n\n        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\n        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\n        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=5) # write the REducelronplateau code here\n        \n        callbacks_list = [checkpoint, LR]\n        \n        \n        # Let us create the train_generator and the val_generator which will be used in .fit_generator.\n        if (num_train_sequences%batch_size) == 0:\n            steps_per_epoch = int(num_train_sequences/batch_size)\n        else:\n            steps_per_epoch = (num_train_sequences//batch_size) + 1\n\n        if (num_val_sequences%batch_size) == 0:\n            validation_steps = int(num_val_sequences/batch_size)\n        else:\n            validation_steps = (num_val_sequences//batch_size) + 1\n       \n        # Let us now fit the model. This will start training the model and with the help of the checkpoints, \n        # you'll be able to save the model at the end of each epoch.\n        history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epoch_num, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n        \n        return history","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:42:57.960630Z","iopub.execute_input":"2022-04-12T12:42:57.961116Z","iopub.status.idle":"2022-04-12T12:42:57.973753Z","shell.execute_reply.started":"2022-04-12T12:42:57.961075Z","shell.execute_reply":"2022-04-12T12:42:57.973015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def run_model_new(model, batch_size = 30, epoch_num=25):\n        \n        # Let us create the train_generator and the val_generator which will be used in .fit_generator.\n        train_generator = generator_new(train_path, train_doc, batch_size)\n        val_generator = generator_new(val_path, val_doc, batch_size)\n        \n\n        model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \n        if not os.path.exists(model_name):\n            os.mkdir(model_name)\n\n        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\n        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\n        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=5) # write the REducelronplateau code here\n        \n        callbacks_list = [checkpoint, LR]\n        \n        \n        # Let us create the train_generator and the val_generator which will be used in .fit_generator.\n        if (num_train_sequences%batch_size) == 0:\n            steps_per_epoch = int(num_train_sequences/batch_size)\n        else:\n            steps_per_epoch = (num_train_sequences//batch_size) + 1\n\n        if (num_val_sequences%batch_size) == 0:\n            validation_steps = int(num_val_sequences/batch_size)\n        else:\n            validation_steps = (num_val_sequences//batch_size) + 1\n       \n        # Let us now fit the model. This will start training the model and with the help of the checkpoints, \n        # you'll be able to save the model at the end of each epoch.\n        history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epoch_num, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n        \n        return history","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:42:57.974875Z","iopub.execute_input":"2022-04-12T12:42:57.975128Z","iopub.status.idle":"2022-04-12T12:42:57.986435Z","shell.execute_reply.started":"2022-04-12T12:42:57.975093Z","shell.execute_reply":"2022-04-12T12:42:57.985719Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Loss function that will be used to plot the graph","metadata":{}},{"cell_type":"code","source":"def loss(history):\n        acc = history.history['categorical_accuracy']\n        val_acc = history.history['val_categorical_accuracy']\n\n        loss = history.history['loss']\n        val_loss = history.history['val_loss']\n\n        epochs_range = range(len(history.history['loss']))\n\n        plt.figure(figsize=(8, 8))\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs_range, acc, label='Training Accuracy')\n        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n        plt.legend(loc='lower right')\n        plt.title('Training and Validation Accuracy')\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs_range, loss, label='Training Loss')\n        plt.plot(epochs_range, val_loss, label='Validation Loss')\n        plt.legend(loc='upper right')\n        plt.title('Training and Validation Loss')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:42:57.987781Z","iopub.execute_input":"2022-04-12T12:42:57.988039Z","iopub.status.idle":"2022-04-12T12:42:57.997929Z","shell.execute_reply.started":"2022-04-12T12:42:57.988006Z","shell.execute_reply":"2022-04-12T12:42:57.997244Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**More parameters**","metadata":{}},{"cell_type":"code","source":"curr_dt_time = datetime.datetime.now()\ntrain_path = '../input/gesture-recognition/train'\nval_path = '../input/gesture-recognition/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 25 # choose the number of epochs\nprint ('# epochs =', num_epochs)\nnum_classes = 5","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1649265013005,"user":{"displayName":"Shubhham Agarwal","userId":"13467995706959391364"},"user_tz":-330},"id":"e2zaXmdASPbr","outputId":"5f3d92cf-049f-45c1-c21c-b61151baac3d","execution":{"iopub.status.busy":"2022-04-12T12:42:58.000867Z","iopub.execute_input":"2022-04-12T12:42:58.001121Z","iopub.status.idle":"2022-04-12T12:42:58.009808Z","shell.execute_reply.started":"2022-04-12T12:42:58.001088Z","shell.execute_reply":"2022-04-12T12:42:58.009005Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Testing Generator function","metadata":{}},{"cell_type":"code","source":"# Test Generator Function\n\nxtrain=generator(train_path,train_doc, 32 )\nx=next(xtrain)\nplt.imshow(x[0][1][1])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:42:58.010976Z","iopub.execute_input":"2022-04-12T12:42:58.011779Z","iopub.status.idle":"2022-04-12T12:43:04.896276Z","shell.execute_reply.started":"2022-04-12T12:42:58.011743Z","shell.execute_reply":"2022-04-12T12:43:04.895601Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Test Crop in Generator Function\n\nxtrain=generator(train_path,train_doc, 32, factor_ht=10, factor_wd=10)\nx=next(xtrain)\nplt.imshow(x[0][1][1])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:43:04.897216Z","iopub.execute_input":"2022-04-12T12:43:04.897466Z","iopub.status.idle":"2022-04-12T12:43:12.058474Z","shell.execute_reply.started":"2022-04-12T12:43:04.897431Z","shell.execute_reply":"2022-04-12T12:43:12.057798Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Define Keras Classes","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.layers import LSTM, GRU, SimpleRNN, LeakyReLU\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"id":"g5m7eXw3m_Dx","execution":{"iopub.status.busy":"2022-04-12T12:43:12.059679Z","iopub.execute_input":"2022-04-12T12:43:12.060001Z","iopub.status.idle":"2022-04-12T12:43:12.384090Z","shell.execute_reply.started":"2022-04-12T12:43:12.059965Z","shell.execute_reply":"2022-04-12T12:43:12.383330Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Developing Models","metadata":{}},{"cell_type":"markdown","source":"### Model 1 \n\n- Vanilla Model","metadata":{}},{"cell_type":"code","source":"#write your model here\nInput_shape = (15, img_size_ht,img_size_wd, 3)\nmodel_1 = Sequential()\nmodel_1.add(Conv3D(8, (3,3,3), padding='same',\n                 input_shape=Input_shape))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Activation('relu'))\n\nmodel_1.add(Conv3D(16, (3, 3,3)))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Activation('relu'))\nmodel_1.add(MaxPooling3D(pool_size=(2, 2,2)))\n\nmodel_1.add(Dropout(0.5))\n\nmodel_1.add(Conv3D(32, (3, 3,3), padding='same'))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Activation('relu'))\n\nmodel_1.add(Conv3D(64, (3, 3,3)))\nmodel_1.add(Activation('relu'))\nmodel_1.add(BatchNormalization())\nmodel_1.add(MaxPooling3D(pool_size=(2, 2,2)))\n\nmodel_1.add(Dropout(0.5))\n\nmodel_1.add(Flatten())\n\nmodel_1.add(Dense(5))\nmodel_1.add(Activation('softmax'))\n\n","metadata":{"id":"sbRZiITuiZRM","execution":{"iopub.status.busy":"2022-04-12T12:43:12.385491Z","iopub.execute_input":"2022-04-12T12:43:12.385744Z","iopub.status.idle":"2022-04-12T12:43:14.877631Z","shell.execute_reply.started":"2022-04-12T12:43:12.385710Z","shell.execute_reply":"2022-04-12T12:43:14.876901Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#write your model here\nInput_shape = (15, img_size_ht,img_size_wd, 3)\n\nlrelu = lambda x: LeakyReLU(alpha=0.1)(x)\n\nmodel_1a = Sequential()\nmodel_1a.add(Conv3D(8, (3,3,3), padding='same',\n                 input_shape=Input_shape))\nmodel_1a.add(BatchNormalization())\nmodel_1a.add(Activation(lrelu))\n\nmodel_1a.add(Conv3D(16, (3, 3,3)))\nmodel_1a.add(BatchNormalization())\nmodel_1a.add(Activation(lrelu))\nmodel_1a.add(MaxPooling3D(pool_size=(2, 2,2)))\n\nmodel_1a.add(Dropout(0.5))\n\nmodel_1a.add(Conv3D(32, (3, 3,3), padding='same'))\nmodel_1a.add(BatchNormalization())\nmodel_1a.add(Activation(lrelu))\n\nmodel_1a.add(Conv3D(64, (3, 3,3)))\nmodel_1a.add(Activation(lrelu))\nmodel_1a.add(BatchNormalization())\nmodel_1a.add(MaxPooling3D(pool_size=(2, 2,2)))\n\nmodel_1a.add(Dropout(0.5))\n\nmodel_1a.add(Flatten())\n\nmodel_1a.add(Dense(5))\nmodel_1a.add(Activation('softmax'))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:43:14.878718Z","iopub.execute_input":"2022-04-12T12:43:14.879493Z","iopub.status.idle":"2022-04-12T12:43:14.985644Z","shell.execute_reply.started":"2022-04-12T12:43:14.879448Z","shell.execute_reply":"2022-04-12T12:43:14.984807Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1.summary())\nhistory= run_model(model_1)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:07:48.659783Z","iopub.execute_input":"2022-04-11T16:07:48.660053Z","iopub.status.idle":"2022-04-11T16:57:46.832413Z","shell.execute_reply.started":"2022-04-11T16:07:48.660019Z","shell.execute_reply":"2022-04-11T16:57:46.831728Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1.summary())\nhistory= run_model(model_1)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:57:46.833649Z","iopub.execute_input":"2022-04-11T16:57:46.834059Z","iopub.status.idle":"2022-04-11T17:47:14.659244Z","shell.execute_reply.started":"2022-04-11T16:57:46.834022Z","shell.execute_reply":"2022-04-11T17:47:14.658567Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_1a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1a.summary())\nhistory= run_model(model_1a)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T17:47:31.059855Z","iopub.execute_input":"2022-04-11T17:47:31.060125Z","iopub.status.idle":"2022-04-11T18:36:06.870502Z","shell.execute_reply.started":"2022-04-11T17:47:31.060094Z","shell.execute_reply":"2022-04-11T18:36:06.869805Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_1a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1a.summary())\nhistory= run_model(model_1a)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:36:06.871942Z","iopub.execute_input":"2022-04-11T18:36:06.872651Z","iopub.status.idle":"2022-04-11T19:24:44.082826Z","shell.execute_reply.started":"2022-04-11T18:36:06.872609Z","shell.execute_reply":"2022-04-11T19:24:44.082160Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noptimiser = Adam(learning_rate=0.001) \nmodel_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1.summary())\nhistory= run_model_new(model_1)\nloss(history)\n# add Codeadd Markdown\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T00:39:26.461118Z","iopub.execute_input":"2022-04-12T00:39:26.461625Z","iopub.status.idle":"2022-04-12T02:08:18.326798Z","shell.execute_reply.started":"2022-04-12T00:39:26.461588Z","shell.execute_reply":"2022-04-12T02:08:18.324102Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1.summary())\nhistory= run_model_new(model_1)\nloss(history)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T02:32:16.265669Z","iopub.execute_input":"2022-04-12T02:32:16.265920Z","iopub.status.idle":"2022-04-12T03:16:51.183970Z","shell.execute_reply.started":"2022-04-12T02:32:16.265892Z","shell.execute_reply":"2022-04-12T03:16:51.183240Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_1a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1a.summary())\nhistory= run_model_new(model_1a)\nloss(history)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:16:51.185521Z","iopub.execute_input":"2022-04-12T03:16:51.185781Z","iopub.status.idle":"2022-04-12T04:00:11.743027Z","shell.execute_reply.started":"2022-04-12T03:16:51.185747Z","shell.execute_reply":"2022-04-12T04:00:11.742268Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\noptimiser = RMSprop(learning_rate=0.001) \nmodel_1a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1a.summary())\nhistory= run_model_new(model_1a)\nloss(history)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:00:11.744426Z","iopub.execute_input":"2022-04-12T04:00:11.744665Z","iopub.status.idle":"2022-04-12T04:44:38.933811Z","shell.execute_reply.started":"2022-04-12T04:00:11.744633Z","shell.execute_reply":"2022-04-12T04:44:38.933123Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"model_7 = Sequential()\n\nmodel_7.add(TimeDistributed(Conv2D(16, (3,3), padding = \"same\"), input_shape=(15,img_size_ht,img_size_wd,3)))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\n           \nmodel_7.add(TimeDistributed(Conv2D(32,(3,3),padding = \"same\")))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\nmodel_7.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_7.add(TimeDistributed(Conv2D(64, (3,3),  padding = \"same\")))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\nmodel_7.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n           \nmodel_7.add(TimeDistributed(Conv2D(128,(3,3), padding = \"same\")))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))       \nmodel_7.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_7.add(TimeDistributed(Conv2D(256,(3,3), padding = \"same\")))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))          \nmodel_7.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\n\nmodel_7.add(TimeDistributed(Flatten()))\nmodel_7.add(GRU(256,dropout=0.2))\nmodel_7.add(Dense(256, activation='relu'))\nmodel_7.add(Dropout(0.2))\n\nmodel_7.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:44:38.935702Z","iopub.execute_input":"2022-04-12T04:44:38.936525Z","iopub.status.idle":"2022-04-12T04:44:39.351891Z","shell.execute_reply.started":"2022-04-12T04:44:38.936485Z","shell.execute_reply":"2022-04-12T04:44:39.351198Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#write your model here\nInput_shape = (15, img_size_ht,img_size_wd, 3)\n\nlrelu = lambda x: LeakyReLU(alpha=0.1)(x)\n\nmodel_7a = Sequential()\n\nmodel_7a.add(TimeDistributed(Conv2D(16, (3,3), padding = \"same\"), input_shape=(15,img_size_ht,img_size_wd,3)))\nmodel_7a.add(TimeDistributed(BatchNormalization()))\nmodel_7a.add(TimeDistributed(Activation(lrelu)))\n           \nmodel_7a.add(TimeDistributed(Conv2D(32,(3,3),padding = \"same\")))\nmodel_7a.add(TimeDistributed(BatchNormalization()))\nmodel_7a.add(TimeDistributed(Activation(lrelu)))\nmodel_7a.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_7a.add(TimeDistributed(Conv2D(64, (3,3),  padding = \"same\")))\nmodel_7a.add(TimeDistributed(BatchNormalization()))\nmodel_7a.add(TimeDistributed(Activation(lrelu)))\nmodel_7a.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n           \nmodel_7a.add(TimeDistributed(Conv2D(128,(3,3), padding = \"same\")))\nmodel_7a.add(TimeDistributed(BatchNormalization()))\nmodel_7a.add(TimeDistributed(Activation(lrelu)))       \nmodel_7a.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_7a.add(TimeDistributed(Conv2D(256,(3,3), padding = \"same\")))\nmodel_7a.add(TimeDistributed(BatchNormalization()))\nmodel_7a.add(TimeDistributed(Activation(lrelu)))          \nmodel_7a.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\n\nmodel_7a.add(TimeDistributed(Flatten()))\nmodel_7a.add(GRU(256,dropout=0.2))\nmodel_7a.add(Dense(256, activation=lrelu))\nmodel_7a.add(Dropout(0.2))\n\nmodel_7a.add(Dense(5, activation='softmax'))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:43:14.987666Z","iopub.execute_input":"2022-04-12T12:43:14.987914Z","iopub.status.idle":"2022-04-12T12:43:15.429126Z","shell.execute_reply.started":"2022-04-12T12:43:14.987881Z","shell.execute_reply":"2022-04-12T12:43:15.428411Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7.summary())\nhistory= run_model(model_7)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:44:39.726934Z","iopub.execute_input":"2022-04-12T04:44:39.727183Z","iopub.status.idle":"2022-04-12T05:34:08.971956Z","shell.execute_reply.started":"2022-04-12T04:44:39.727150Z","shell.execute_reply":"2022-04-12T05:34:08.971252Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7.summary())\nhistory= run_model(model_7)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T05:34:08.973355Z","iopub.execute_input":"2022-04-12T05:34:08.973987Z","iopub.status.idle":"2022-04-12T06:23:16.233955Z","shell.execute_reply.started":"2022-04-12T05:34:08.973945Z","shell.execute_reply":"2022-04-12T06:23:16.233278Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model(model_7a)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T06:23:16.235324Z","iopub.execute_input":"2022-04-12T06:23:16.235585Z","iopub.status.idle":"2022-04-12T07:12:46.196325Z","shell.execute_reply.started":"2022-04-12T06:23:16.235551Z","shell.execute_reply":"2022-04-12T07:12:46.195612Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model(model_7a)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T07:12:46.197668Z","iopub.execute_input":"2022-04-12T07:12:46.198075Z","iopub.status.idle":"2022-04-12T08:03:15.732547Z","shell.execute_reply.started":"2022-04-12T07:12:46.198036Z","shell.execute_reply":"2022-04-12T08:03:15.731823Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noptimiser = Adam(learning_rate=0.001) \nmodel_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7.summary())\nhistory= run_model_new(model_7)\nloss(history)\n# add Codeadd Markdown\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T08:03:15.734937Z","iopub.execute_input":"2022-04-12T08:03:15.735349Z","iopub.status.idle":"2022-04-12T08:47:44.119948Z","shell.execute_reply.started":"2022-04-12T08:03:15.735310Z","shell.execute_reply":"2022-04-12T08:47:44.119277Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7.summary())\nhistory= run_model_new(model_7)\nloss(history)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T08:47:44.121454Z","iopub.execute_input":"2022-04-12T08:47:44.121919Z","iopub.status.idle":"2022-04-12T09:32:13.059829Z","shell.execute_reply.started":"2022-04-12T08:47:44.121881Z","shell.execute_reply":"2022-04-12T09:32:13.059090Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model_new(model_7a)\nloss(history)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T09:32:13.061105Z","iopub.execute_input":"2022-04-12T09:32:13.061439Z","iopub.status.idle":"2022-04-12T10:16:33.740335Z","shell.execute_reply.started":"2022-04-12T09:32:13.061404Z","shell.execute_reply":"2022-04-12T10:16:33.739532Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"\noptimiser = RMSprop(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model_new(model_7a)\nloss(history)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:16:33.741522Z","iopub.execute_input":"2022-04-12T10:16:33.742550Z","iopub.status.idle":"2022-04-12T10:59:38.499036Z","shell.execute_reply.started":"2022-04-12T10:16:33.742511Z","shell.execute_reply":"2022-04-12T10:59:38.497730Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model(model_7a)\nloss(history) ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:43:27.596522Z","iopub.execute_input":"2022-04-12T12:43:27.596773Z","iopub.status.idle":"2022-04-12T13:33:58.316837Z","shell.execute_reply.started":"2022-04-12T12:43:27.596746Z","shell.execute_reply":"2022-04-12T13:33:58.316169Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model(model_7a)\nloss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:33:58.318688Z","iopub.execute_input":"2022-04-12T13:33:58.319146Z","iopub.status.idle":"2022-04-12T14:22:48.646887Z","shell.execute_reply.started":"2022-04-12T13:33:58.319109Z","shell.execute_reply":"2022-04-12T14:22:48.646206Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model_new(model_7a)\nloss(history)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:22:48.648277Z","iopub.execute_input":"2022-04-12T14:22:48.648738Z","iopub.status.idle":"2022-04-12T15:06:38.517771Z","shell.execute_reply.started":"2022-04-12T14:22:48.648701Z","shell.execute_reply":"2022-04-12T15:06:38.517095Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_7a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7a.summary())\nhistory= run_model_new(model_7a)\nloss(history)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:06:38.519035Z","iopub.execute_input":"2022-04-12T15:06:38.519428Z","iopub.status.idle":"2022-04-12T15:51:07.375171Z","shell.execute_reply.started":"2022-04-12T15:06:38.519387Z","shell.execute_reply":"2022-04-12T15:51:07.374445Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:51:18.525878Z","iopub.execute_input":"2022-04-12T15:51:18.526128Z","iopub.status.idle":"2022-04-12T15:51:18.530958Z","shell.execute_reply.started":"2022-04-12T15:51:18.526099Z","shell.execute_reply":"2022-04-12T15:51:18.530265Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:51:31.034068Z","iopub.execute_input":"2022-04-12T15:51:31.034313Z","iopub.status.idle":"2022-04-12T15:51:31.256600Z","shell.execute_reply.started":"2022-04-12T15:51:31.034287Z","shell.execute_reply":"2022-04-12T15:51:31.255549Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\n# setup backend for matplotlibs plots\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:53:19.853622Z","iopub.execute_input":"2022-04-12T15:53:19.854487Z","iopub.status.idle":"2022-04-12T15:53:20.067565Z","shell.execute_reply.started":"2022-04-12T15:53:19.854440Z","shell.execute_reply":"2022-04-12T15:53:20.066825Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"This is Complete Overfit. But seeing the ipward trend in accracy and Validation Loss, running the model for more epoch we could see improvement in Validation parameters.","metadata":{}}]}