{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport datetime\nimport os\nimport matplotlib.pyplot as plt\nfrom imageio import imread\nfrom skimage import io, transform\nfrom skimage.transform import resize\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"OcPsZCLlR_G4","execution":{"iopub.status.busy":"2022-04-10T05:37:46.528451Z","iopub.execute_input":"2022-04-10T05:37:46.528851Z","iopub.status.idle":"2022-04-10T05:37:47.928448Z","shell.execute_reply.started":"2022-04-10T05:37:46.528757Z","shell.execute_reply":"2022-04-10T05:37:47.927537Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)","metadata":{"id":"WRRzV3bgSN0l","execution":{"iopub.status.busy":"2022-04-10T05:37:51.954106Z","iopub.execute_input":"2022-04-10T05:37:51.954382Z","iopub.status.idle":"2022-04-10T05:37:57.031239Z","shell.execute_reply.started":"2022-04-10T05:37:51.954350Z","shell.execute_reply":"2022-04-10T05:37:57.030378Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_doc = np.random.permutation(open('../input/gesture-recognition/train.csv').readlines())\nval_doc = np.random.permutation(open('../input/gesture-recognition/val.csv').readlines())\n\n# Batch size handled in class\n\nbatch_size = 30\nimg_size_ht = 120\nimg_size_wd = 120","metadata":{"id":"dF8BzXNCSOlU","execution":{"iopub.status.busy":"2022-04-10T05:38:15.070199Z","iopub.execute_input":"2022-04-10T05:38:15.070467Z","iopub.status.idle":"2022-04-10T05:38:15.090911Z","shell.execute_reply.started":"2022-04-10T05:38:15.070433Z","shell.execute_reply":"2022-04-10T05:38:15.090118Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size, factor_ht=0, factor_wd=0 ):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    \n    img_idx = np.arange(0,30,2) #create a list of image numbers you want to use for a particular video\n    \n    while True:\n        t = np.random.permutation(folder_list)\n\n\n        num_batches = len(folder_list)//batch_size # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                   \n                    # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n        rem_image = len(folder_list)%batch_size\n        batch += 1\n        if(rem_image!=0):\n            batch_data = np.zeros((rem_image,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n            for folder in range(rem_image): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    \n                   # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels\n            ","metadata":{"id":"l9HmSX-ZSPYA","execution":{"iopub.status.busy":"2022-04-10T05:38:18.264852Z","iopub.execute_input":"2022-04-10T05:38:18.265325Z","iopub.status.idle":"2022-04-10T05:38:18.288466Z","shell.execute_reply.started":"2022-04-10T05:38:18.265274Z","shell.execute_reply":"2022-04-10T05:38:18.287740Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def loss(history):\n        acc = history.history['categorical_accuracy']\n        val_acc = history.history['val_categorical_accuracy']\n\n        loss = history.history['loss']\n        val_loss = history.history['val_loss']\n\n        epochs_range = range(len(history.history['loss']))\n\n        plt.figure(figsize=(8, 8))\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs_range, acc, label='Training Accuracy')\n        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n        plt.legend(loc='lower right')\n        plt.title('Training and Validation Accuracy')\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs_range, loss, label='Training Loss')\n        plt.plot(epochs_range, val_loss, label='Validation Loss')\n        plt.legend(loc='upper right')\n        plt.title('Training and Validation Loss')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T05:38:30.388347Z","iopub.execute_input":"2022-04-10T05:38:30.388644Z","iopub.status.idle":"2022-04-10T05:38:30.403246Z","shell.execute_reply.started":"2022-04-10T05:38:30.388612Z","shell.execute_reply":"2022-04-10T05:38:30.401502Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"curr_dt_time = datetime.datetime.now()\ntrain_path = '../input/gesture-recognition/train'\nval_path = '../input/gesture-recognition/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 10 # choose the number of epochs\nprint ('# epochs =', num_epochs)\nnum_classes = 5","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1649265013005,"user":{"displayName":"Shubhham Agarwal","userId":"13467995706959391364"},"user_tz":-330},"id":"e2zaXmdASPbr","outputId":"5f3d92cf-049f-45c1-c21c-b61151baac3d","execution":{"iopub.status.busy":"2022-04-10T05:38:37.252443Z","iopub.execute_input":"2022-04-10T05:38:37.252743Z","iopub.status.idle":"2022-04-10T05:38:37.260449Z","shell.execute_reply.started":"2022-04-10T05:38:37.252687Z","shell.execute_reply":"2022-04-10T05:38:37.259667Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Testing Generator function","metadata":{}},{"cell_type":"code","source":"# Test Generator Function\n\nxtrain=generator(train_path,train_doc, 32 )\nx=next(xtrain)\nplt.imshow(x[0][1][1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:56:54.596819Z","iopub.execute_input":"2022-04-10T02:56:54.597209Z","iopub.status.idle":"2022-04-10T02:57:00.286650Z","shell.execute_reply.started":"2022-04-10T02:56:54.597070Z","shell.execute_reply":"2022-04-10T02:57:00.283810Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Test Crop in Generator Function\n\nxtrain=generator(train_path,train_doc, 32, factor_ht=10, factor_wd=10)\nx=next(xtrain)\nplt.imshow(x[0][1][1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:57:00.287995Z","iopub.execute_input":"2022-04-10T02:57:00.288265Z","iopub.status.idle":"2022-04-10T02:57:06.395772Z","shell.execute_reply.started":"2022-04-10T02:57:00.288229Z","shell.execute_reply":"2022-04-10T02:57:06.395122Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Keras Classes","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.layers import LSTM, GRU, SimpleRNN, LeakyReLU\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"id":"g5m7eXw3m_Dx","execution":{"iopub.status.busy":"2022-04-10T05:38:49.698955Z","iopub.execute_input":"2022-04-10T05:38:49.699227Z","iopub.status.idle":"2022-04-10T05:38:50.031655Z","shell.execute_reply.started":"2022-04-10T05:38:49.699189Z","shell.execute_reply":"2022-04-10T05:38:50.030868Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Model 1","metadata":{}},{"cell_type":"code","source":"#write your model here\n\nInput_shape = (15, img_size_ht,img_size_wd, 3)\nmodel_1 = Sequential()\nmodel_1.add(Conv3D(32, (3,3,3), padding='same',\n                 input_shape=Input_shape))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Activation('relu'))\n\nmodel_1.add(Conv3D(32, (3, 3,3)))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Activation('relu'))\nmodel_1.add(MaxPooling3D(pool_size=(2, 2,2)))\n\nmodel_1.add(Dropout(0.5))\n\nmodel_1.add(Conv3D(64, (3, 3,3), padding='same'))\nmodel_1.add(BatchNormalization())\nmodel_1.add(Activation('relu'))\nmodel_1.add(Conv3D(64, (3, 3,3)))\n\nmodel_1.add(Activation('relu'))\nmodel_1.add(BatchNormalization())\nmodel_1.add(MaxPooling3D(pool_size=(2, 2,2)))\n\nmodel_1.add(Dropout(0.5))\n\nmodel_1.add(Flatten())\n\nmodel_1.add(Dense(5))\nmodel_1.add(Activation('softmax'))","metadata":{"id":"sbRZiITuiZRM","execution":{"iopub.status.busy":"2022-04-10T07:35:50.587469Z","iopub.execute_input":"2022-04-10T07:35:50.587774Z","iopub.status.idle":"2022-04-10T07:35:50.699995Z","shell.execute_reply.started":"2022-04-10T07:35:50.587736Z","shell.execute_reply":"2022-04-10T07:35:50.699177Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_1.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:35:56.016200Z","iopub.execute_input":"2022-04-10T07:35:56.016475Z","iopub.status.idle":"2022-04-10T07:35:56.037492Z","shell.execute_reply.started":"2022-04-10T07:35:56.016444Z","shell.execute_reply":"2022-04-10T07:35:56.036842Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:57:09.090870Z","iopub.execute_input":"2022-04-10T02:57:09.091542Z","iopub.status.idle":"2022-04-10T03:16:05.908225Z","shell.execute_reply.started":"2022-04-10T02:57:09.091507Z","shell.execute_reply":"2022-04-10T03:16:05.907453Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:16:05.909518Z","iopub.execute_input":"2022-04-10T03:16:05.909927Z","iopub.status.idle":"2022-04-10T03:16:06.356709Z","shell.execute_reply.started":"2022-04-10T03:16:05.909889Z","shell.execute_reply":"2022-04-10T03:16:06.356045Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Model 2","metadata":{}},{"cell_type":"code","source":"# Define Model\n\nlrelu = lambda x: LeakyReLU(alpha=0.1)(x)\n\nmodel_2 = Sequential()\n\nmodel_2.add(Conv3D(16,(3,3,3),  padding='same', input_shape= (15,100,100,3) ))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation(lrelu))\nmodel_2.add(Conv3D(16,(3,3,3),padding='same'))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation(lrelu))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_2.add(Conv3D(32,(3,3,3),padding='same'))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation(lrelu))\nmodel_2.add(Conv3D(32,(3,3,3),padding='same'))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation(lrelu))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_2.add(Conv3D(64,(3,3,3),padding='same'))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation(lrelu))\nmodel_2.add(Conv3D(64,(3,3,3),padding='same'))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation(lrelu))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_2.add(Flatten())\n\nmodel_2.add(Dense(64))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation('elu'))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Dense(32))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation('elu'))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:16:06.357804Z","iopub.execute_input":"2022-04-10T03:16:06.359430Z","iopub.status.idle":"2022-04-10T03:16:06.565125Z","shell.execute_reply.started":"2022-04-10T03:16:06.359389Z","shell.execute_reply":"2022-04-10T03:16:06.564442Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_2.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:16:06.566233Z","iopub.execute_input":"2022-04-10T03:16:06.566490Z","iopub.status.idle":"2022-04-10T03:16:06.592972Z","shell.execute_reply.started":"2022-04-10T03:16:06.566458Z","shell.execute_reply":"2022-04-10T03:16:06.592315Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:16:06.595742Z","iopub.execute_input":"2022-04-10T03:16:06.595926Z","iopub.status.idle":"2022-04-10T03:33:37.745579Z","shell.execute_reply.started":"2022-04-10T03:16:06.595904Z","shell.execute_reply":"2022-04-10T03:33:37.744761Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:33:37.746963Z","iopub.execute_input":"2022-04-10T03:33:37.747572Z","iopub.status.idle":"2022-04-10T03:33:38.052876Z","shell.execute_reply.started":"2022-04-10T03:33:37.747524Z","shell.execute_reply":"2022-04-10T03:33:38.052128Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Model 3","metadata":{}},{"cell_type":"code","source":"# Define Model\n\nmodel_3 = Sequential()\n\nmodel_3.add(Conv3D(8,(3,3,3),   padding='same', input_shape= (15,img_size_ht,img_size_wd,3) ))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_3.add(Conv3D(16,(3,3,3),padding='same'))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_3.add(Conv3D(32,(1,3,3), padding='same'))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_3.add(Conv3D(64,(1,2,2), padding='same'))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling3D(pool_size= (1,3,3)))\n\nmodel_3.add(Flatten())\n\nmodel_3.add(Dense(1000, activation='relu'))\nmodel_3.add(Dropout(0.5))\n\nmodel_3.add(Dense(500, activation='relu'))\nmodel_3.add(Dropout(0.5))\n\nmodel_3.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:18:50.400024Z","iopub.execute_input":"2022-04-10T08:18:50.400297Z","iopub.status.idle":"2022-04-10T08:18:50.532872Z","shell.execute_reply.started":"2022-04-10T08:18:50.400267Z","shell.execute_reply":"2022-04-10T08:18:50.532103Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"optimiser = RMSprop(learning_rate=0.001) \nmodel_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_3.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:18:56.170884Z","iopub.execute_input":"2022-04-10T08:18:56.171442Z","iopub.status.idle":"2022-04-10T08:18:56.195870Z","shell.execute_reply.started":"2022-04-10T08:18:56.171404Z","shell.execute_reply":"2022-04-10T08:18:56.195035Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:05:43.165692Z","iopub.execute_input":"2022-04-10T04:05:43.165955Z","iopub.status.idle":"2022-04-10T04:24:15.873604Z","shell.execute_reply.started":"2022-04-10T04:05:43.165926Z","shell.execute_reply":"2022-04-10T04:24:15.872732Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:24:28.529701Z","iopub.execute_input":"2022-04-10T04:24:28.529949Z","iopub.status.idle":"2022-04-10T04:24:28.809749Z","shell.execute_reply.started":"2022-04-10T04:24:28.529921Z","shell.execute_reply":"2022-04-10T04:24:28.809092Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Model 4","metadata":{}},{"cell_type":"code","source":"# Define Model\n\nmodel_4 = Sequential()\n\nmodel_4.add(Conv3D(8,(3,3,3),   padding='same', input_shape= (15,img_size_ht,img_size_wd,3) ))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(Conv3D(16,(3,3,3),padding='same', activation='relu'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_4.add(Conv3D(16,(3,3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(Conv3D(32,(3,3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_4.add(Conv3D(32,(1,3,3), padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\nmodel_4.add(Conv3D(64,(3,3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_4.add(Conv3D(64,(1,3,3), padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(Conv3D(128,(3,3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\n\nmodel_4.add(MaxPooling3D(pool_size= (1,3,3)))\n\nmodel_4.add(Flatten())\n\nmodel_4.add(Dense(1000))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.25))\n\nmodel_4.add(Dense(500))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.25))\n\nmodel_4.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:25:12.444997Z","iopub.execute_input":"2022-04-10T04:25:12.447381Z","iopub.status.idle":"2022-04-10T04:25:12.883383Z","shell.execute_reply.started":"2022-04-10T04:25:12.447336Z","shell.execute_reply":"2022-04-10T04:25:12.882572Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_4.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:25:12.888397Z","iopub.execute_input":"2022-04-10T04:25:12.890522Z","iopub.status.idle":"2022-04-10T04:25:12.934997Z","shell.execute_reply.started":"2022-04-10T04:25:12.890484Z","shell.execute_reply":"2022-04-10T04:25:12.934221Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:25:12.938958Z","iopub.execute_input":"2022-04-10T04:25:12.941097Z","iopub.status.idle":"2022-04-10T04:43:44.732472Z","shell.execute_reply.started":"2022-04-10T04:25:12.941059Z","shell.execute_reply":"2022-04-10T04:43:44.731633Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:43:44.735001Z","iopub.execute_input":"2022-04-10T04:43:44.735359Z","iopub.status.idle":"2022-04-10T04:43:45.079562Z","shell.execute_reply.started":"2022-04-10T04:43:44.735321Z","shell.execute_reply":"2022-04-10T04:43:45.078740Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Model 5","metadata":{}},{"cell_type":"code","source":"# Define Model\n\nmodel_5 = Sequential()\n\nmodel_5.add(Conv3D(8,(3,3,3),   padding='same', input_shape= (15,img_size_ht,img_size_wd,3) ))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(Conv3D(16,(3,3,3),padding='same', activation='relu'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_5.add(Conv3D(16,(3,3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(Conv3D(32,(3,3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_5.add(Conv3D(32,(3,3,3), padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\nmodel_5.add(Conv3D(64,(3,3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_5.add(Conv3D(64,(3,3,3), padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(Conv3D(128,(3,3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\n\nmodel_5.add(MaxPooling3D(pool_size= (1,2,2)))\n\nmodel_5.add(Flatten())\n\nmodel_5.add(Dense(1000))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.25))\n\nmodel_5.add(Dense(500))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_3.add(Dropout(0.25))\n\nmodel_5.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:49:03.803767Z","iopub.execute_input":"2022-04-10T04:49:03.804027Z","iopub.status.idle":"2022-04-10T04:49:04.197860Z","shell.execute_reply.started":"2022-04-10T04:49:03.803997Z","shell.execute_reply":"2022-04-10T04:49:04.197137Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_5.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:49:04.199652Z","iopub.execute_input":"2022-04-10T04:49:04.200135Z","iopub.status.idle":"2022-04-10T04:49:04.239069Z","shell.execute_reply.started":"2022-04-10T04:49:04.200097Z","shell.execute_reply":"2022-04-10T04:49:04.238349Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T04:49:04.240583Z","iopub.execute_input":"2022-04-10T04:49:04.241050Z","iopub.status.idle":"2022-04-10T05:07:11.665182Z","shell.execute_reply.started":"2022-04-10T04:49:04.241014Z","shell.execute_reply":"2022-04-10T05:07:11.664474Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T05:07:11.667545Z","iopub.execute_input":"2022-04-10T05:07:11.667812Z","iopub.status.idle":"2022-04-10T05:07:11.955423Z","shell.execute_reply.started":"2022-04-10T05:07:11.667776Z","shell.execute_reply":"2022-04-10T05:07:11.954766Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### Model 6","metadata":{"id":"bfF-4RgUVs8T"}},{"cell_type":"code","source":"# Define Model\n\nmodel_6 = Sequential()\n\nmodel_6.add(Conv3D(32,(2,2,2), activation='elu', input_shape=(15,img_size_ht,img_size_wd,3), padding = \"same\" ))\nmodel_6.add(Conv3D(32,(2,2,2),  activation='elu'))\nmodel_6.add(Dropout(0.5))\nmodel_6.add(BatchNormalization())\nmodel_6.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_6.add(Conv3D(64,(2,2,2), activation='elu', padding = \"same\" ))\nmodel_6.add(Conv3D(64,(2,2,2),  activation='elu'))\nmodel_6.add(Dropout(0.5))\nmodel_6.add(BatchNormalization())\nmodel_6.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_6.add(Conv3D(128,(2,2,2), activation='elu', padding = \"same\" ))\nmodel_6.add(Conv3D(128,(2,2,2),  activation='elu'))\nmodel_6.add(Dropout(0.5))\nmodel_6.add(BatchNormalization())\nmodel_6.add(MaxPooling3D(pool_size= (2,2,2)))\n\nmodel_6.add(Flatten())\n\nmodel_6.add(Dense(128, activation='elu'))\nmodel_6.add(Dense(64, activation='elu'))\nmodel_6.add(Dense(5, activation='softmax'))","metadata":{"id":"iPr6r4QqVtAF","execution":{"iopub.status.busy":"2022-04-10T05:39:48.247157Z","iopub.execute_input":"2022-04-10T05:39:48.247429Z","iopub.status.idle":"2022-04-10T05:39:50.754130Z","shell.execute_reply.started":"2022-04-10T05:39:48.247399Z","shell.execute_reply":"2022-04-10T05:39:50.753266Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"optimiser = SGD(learning_rate=0.001) \nmodel_6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_6.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T05:40:17.443953Z","iopub.execute_input":"2022-04-10T05:40:17.444209Z","iopub.status.idle":"2022-04-10T05:40:17.469108Z","shell.execute_reply.started":"2022-04-10T05:40:17.444180Z","shell.execute_reply":"2022-04-10T05:40:17.468432Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"batch_size=30\n\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_6.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T05:40:25.607343Z","iopub.execute_input":"2022-04-10T05:40:25.607618Z","iopub.status.idle":"2022-04-10T05:59:57.971108Z","shell.execute_reply.started":"2022-04-10T05:40:25.607588Z","shell.execute_reply":"2022-04-10T05:59:57.970354Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:00:25.372631Z","iopub.execute_input":"2022-04-10T06:00:25.373262Z","iopub.status.idle":"2022-04-10T06:00:25.714347Z","shell.execute_reply.started":"2022-04-10T06:00:25.373222Z","shell.execute_reply":"2022-04-10T06:00:25.713655Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Model 7","metadata":{"id":"iFGKzVneVtH9"}},{"cell_type":"code","source":"model_7 = Sequential()\n\nmodel_7.add(TimeDistributed(Conv2D(32, (3,3), padding = \"same\"), input_shape=(15,img_size_ht,img_size_wd,3)))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\nmodel_7.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_7.add(TimeDistributed(Conv2D(32,(3,3))))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\nmodel_7.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_7.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_7.add(TimeDistributed(Conv2D(64, (3,3),  padding = \"same\")))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\nmodel_7.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_7.add(TimeDistributed(Conv2D(64,(3,3))))\nmodel_7.add(TimeDistributed(BatchNormalization()))\nmodel_7.add(TimeDistributed(Activation('relu')))\nmodel_7.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_7.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_7.add(TimeDistributed(Flatten()))\nmodel_7.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5))\n\nmodel_7.add(Dense(5, activation='softmax'))","metadata":{"id":"K25I1QvfVtL0","execution":{"iopub.status.busy":"2022-04-10T06:04:57.208937Z","iopub.execute_input":"2022-04-10T06:04:57.209199Z","iopub.status.idle":"2022-04-10T06:04:57.693470Z","shell.execute_reply.started":"2022-04-10T06:04:57.209168Z","shell.execute_reply":"2022-04-10T06:04:57.692669Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_7.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:05:49.033665Z","iopub.execute_input":"2022-04-10T06:05:49.034354Z","iopub.status.idle":"2022-04-10T06:05:49.058123Z","shell.execute_reply.started":"2022-04-10T06:05:49.034317Z","shell.execute_reply":"2022-04-10T06:05:49.057360Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_7.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:05:54.995431Z","iopub.execute_input":"2022-04-10T06:05:54.995687Z","iopub.status.idle":"2022-04-10T06:24:52.419686Z","shell.execute_reply.started":"2022-04-10T06:05:54.995656Z","shell.execute_reply":"2022-04-10T06:24:52.418943Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:25:04.259422Z","iopub.execute_input":"2022-04-10T06:25:04.259992Z","iopub.status.idle":"2022-04-10T06:25:04.605728Z","shell.execute_reply.started":"2022-04-10T06:25:04.259952Z","shell.execute_reply":"2022-04-10T06:25:04.604970Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Model 8","metadata":{"id":"GSsv7JQpSPkG"}},{"cell_type":"code","source":"model_8 = Sequential()\n\nmodel_8.add(TimeDistributed(Conv2D(32, (3,3), padding = \"same\"), input_shape=(15,img_size_ht,img_size_wd,3)))\nmodel_8.add(TimeDistributed(BatchNormalization()))\nmodel_8.add(TimeDistributed(Activation('relu')))\nmodel_8.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_8.add(TimeDistributed(Conv2D(32,(3,3))))\nmodel_8.add(TimeDistributed(BatchNormalization()))\nmodel_8.add(TimeDistributed(Activation('relu')))\nmodel_8.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_8.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_8.add(TimeDistributed(Conv2D(64, (3,3),  padding = \"same\")))\nmodel_8.add(TimeDistributed(BatchNormalization()))\nmodel_8.add(TimeDistributed(Activation('relu')))\nmodel_8.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_8.add(TimeDistributed(Conv2D(64,(3,3))))\nmodel_8.add(TimeDistributed(BatchNormalization()))\nmodel_8.add(TimeDistributed(Activation('relu')))\nmodel_8.add(TimeDistributed(Dropout(0.5)))\n            \nmodel_8.add(TimeDistributed(MaxPooling2D(pool_size= (2,2))))\n\nmodel_8.add(TimeDistributed(Flatten()))\nmodel_8.add(GRU(64,dropout=0.5, recurrent_dropout=0.5))\n\nmodel_8.add(Dense(5, activation='softmax'))","metadata":{"id":"fYm7y51sSPnn","execution":{"iopub.status.busy":"2022-04-10T06:26:26.652851Z","iopub.execute_input":"2022-04-10T06:26:26.653377Z","iopub.status.idle":"2022-04-10T06:26:26.959972Z","shell.execute_reply.started":"2022-04-10T06:26:26.653339Z","shell.execute_reply":"2022-04-10T06:26:26.959240Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_8.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_8.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:26:30.659673Z","iopub.execute_input":"2022-04-10T06:26:30.660393Z","iopub.status.idle":"2022-04-10T06:26:30.683051Z","shell.execute_reply.started":"2022-04-10T06:26:30.660353Z","shell.execute_reply":"2022-04-10T06:26:30.682347Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_8.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:26:45.607276Z","iopub.execute_input":"2022-04-10T06:26:45.607571Z","iopub.status.idle":"2022-04-10T06:46:14.265804Z","shell.execute_reply.started":"2022-04-10T06:26:45.607539Z","shell.execute_reply":"2022-04-10T06:46:14.264951Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:46:14.267764Z","iopub.execute_input":"2022-04-10T06:46:14.268049Z","iopub.status.idle":"2022-04-10T06:46:14.577525Z","shell.execute_reply.started":"2022-04-10T06:46:14.268015Z","shell.execute_reply":"2022-04-10T06:46:14.576774Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Model 9","metadata":{"id":"HyD9VR5wFIQw"}},{"cell_type":"code","source":"model_9 = Sequential()\nmodel_9.add(TimeDistributed(EfficientNetB0(weights='imagenet', include_top=False),input_shape=(15,img_size_ht,img_size_wd,3)))\n\nfor layer in model_9.layers:\n    layer.trainable = False\n\nmodel_9.add(TimeDistributed(Flatten()))    \nmodel_9.add(GRU(128,dropout=0.5, recurrent_dropout=0.5))\nmodel_9.add(Dense(5,activation='softmax'))","metadata":{"id":"SCVGNjg9FIrr","execution":{"iopub.status.busy":"2022-04-10T06:46:39.774817Z","iopub.execute_input":"2022-04-10T06:46:39.775531Z","iopub.status.idle":"2022-04-10T06:46:42.638953Z","shell.execute_reply.started":"2022-04-10T06:46:39.775480Z","shell.execute_reply":"2022-04-10T06:46:42.638160Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_9.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_9.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:46:49.135595Z","iopub.execute_input":"2022-04-10T06:46:49.136140Z","iopub.status.idle":"2022-04-10T06:46:49.169750Z","shell.execute_reply.started":"2022-04-10T06:46:49.136100Z","shell.execute_reply":"2022-04-10T06:46:49.168998Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_9.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:46:54.747685Z","iopub.execute_input":"2022-04-10T06:46:54.748275Z","iopub.status.idle":"2022-04-10T07:06:29.831868Z","shell.execute_reply.started":"2022-04-10T06:46:54.748240Z","shell.execute_reply":"2022-04-10T07:06:29.831049Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:09:03.007928Z","iopub.execute_input":"2022-04-10T07:09:03.008196Z","iopub.status.idle":"2022-04-10T07:09:03.321075Z","shell.execute_reply.started":"2022-04-10T07:09:03.008165Z","shell.execute_reply":"2022-04-10T07:09:03.320391Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Model 10","metadata":{"id":"NGUsf8OqFI-Z"}},{"cell_type":"code","source":"model_10 = Sequential()\nmodel_10.add(TimeDistributed(EfficientNetB0(weights='imagenet', include_top=False),input_shape=(15,img_size_ht,img_size_wd,3)))\n\nfor layer in model_10.layers:\n    layer.trainable = False\n\nmodel_10.add(TimeDistributed(Flatten()))    \nmodel_10.add(GRU(512,dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\nmodel_10.add(GRU(256,dropout=0.5, recurrent_dropout=0.5))\nmodel_10.add(Dense(5,activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:11:52.040499Z","iopub.execute_input":"2022-04-10T07:11:52.040826Z","iopub.status.idle":"2022-04-10T07:11:55.144654Z","shell.execute_reply.started":"2022-04-10T07:11:52.040781Z","shell.execute_reply":"2022-04-10T07:11:55.143921Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"optimiser = Adam(learning_rate=0.001) \nmodel_10.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_10.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:11:57.134680Z","iopub.execute_input":"2022-04-10T07:11:57.135388Z","iopub.status.idle":"2022-04-10T07:11:57.166155Z","shell.execute_reply.started":"2022-04-10T07:11:57.135349Z","shell.execute_reply":"2022-04-10T07:11:57.165365Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_10.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:13:22.494542Z","iopub.execute_input":"2022-04-10T07:13:22.494823Z","iopub.status.idle":"2022-04-10T07:32:40.346561Z","shell.execute_reply.started":"2022-04-10T07:13:22.494793Z","shell.execute_reply":"2022-04-10T07:32:40.345662Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:33:48.537160Z","iopub.execute_input":"2022-04-10T07:33:48.537458Z","iopub.status.idle":"2022-04-10T07:33:48.837849Z","shell.execute_reply.started":"2022-04-10T07:33:48.537426Z","shell.execute_reply":"2022-04-10T07:33:48.837047Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Redefining Generator Function","metadata":{}},{"cell_type":"code","source":"def generator_new(source_path, folder_list, batch_size, factor_ht=0, factor_wd=0 ):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    \n    img_idx = np.arange(0,30,2) #create a list of image numbers you want to use for a particular video\n    \n    while True:\n        t = np.random.permutation(folder_list)\n\n\n        num_batches = len(folder_list)//batch_size # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                   \n                    # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n        rem_image = len(folder_list)%batch_size\n        batch += 1\n        if(rem_image!=0):\n            batch_data = np.zeros((rem_image,len(img_idx),img_size_ht,img_size_wd,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n            for folder in range(rem_image): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    \n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    \n                   # CROP \n                    if factor_ht!=0 or factor_wd!=0:\n                        image_ht, image_wd, _ = image.shape\n                        ht1 = int(factor_ht / 2)\n                        ht2 = image_ht - int(factor_ht /2)\n                        wd1 = int(factor_wd / 2)\n                        wd2 = image_wd - int(factor_wd / 2)\n                        image=image[ht1:ht2, wd1:wd2,:]\n                    \n                    # Resize and Normalize\n                    image = resize(image,(img_size_ht,img_size_wd))\n                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:34:04.041581Z","iopub.execute_input":"2022-04-10T07:34:04.042013Z","iopub.status.idle":"2022-04-10T07:34:04.063421Z","shell.execute_reply.started":"2022-04-10T07:34:04.041977Z","shell.execute_reply":"2022-04-10T07:34:04.062738Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"#### Of all the models ran, we will try out the models that showed the most promising results and weren't overfitting. <br><br>Also increasing the num_epochs to 20","metadata":{}},{"cell_type":"code","source":"num_epochs=20","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:34:10.099370Z","iopub.execute_input":"2022-04-10T07:34:10.099652Z","iopub.status.idle":"2022-04-10T07:34:10.103321Z","shell.execute_reply.started":"2022-04-10T07:34:10.099620Z","shell.execute_reply":"2022-04-10T07:34:10.102656Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Re-running Model 1","metadata":{}},{"cell_type":"code","source":"train_generator = generator_new(train_path, train_doc, batch_size)\nval_generator = generator_new(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T07:36:06.360418Z","iopub.execute_input":"2022-04-10T07:36:06.360716Z","iopub.status.idle":"2022-04-10T08:18:33.518729Z","shell.execute_reply.started":"2022-04-10T07:36:06.360669Z","shell.execute_reply":"2022-04-10T08:18:33.517865Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:18:34.467687Z","iopub.execute_input":"2022-04-10T08:18:34.468221Z","iopub.status.idle":"2022-04-10T08:18:34.778372Z","shell.execute_reply.started":"2022-04-10T08:18:34.468175Z","shell.execute_reply":"2022-04-10T08:18:34.777606Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### Model 3 - Re-run","metadata":{}},{"cell_type":"code","source":"train_generator = generator_new(train_path, train_doc, batch_size)\nval_generator = generator_new(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T09:36:33.464553Z","iopub.execute_input":"2022-04-10T09:36:33.465353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:55:32.710825Z","iopub.execute_input":"2022-04-10T08:55:32.711192Z","iopub.status.idle":"2022-04-10T08:55:33.009246Z","shell.execute_reply.started":"2022-04-10T08:55:32.711150Z","shell.execute_reply":"2022-04-10T08:55:33.008561Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### Model 7 - Re-run","metadata":{}},{"cell_type":"code","source":"train_generator = generator_new(train_path, train_doc, batch_size)\nval_generator = generator_new(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_7.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:55:50.086939Z","iopub.execute_input":"2022-04-10T08:55:50.087252Z","iopub.status.idle":"2022-04-10T09:33:54.506228Z","shell.execute_reply.started":"2022-04-10T08:55:50.087216Z","shell.execute_reply":"2022-04-10T09:33:54.505406Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T09:33:54.821836Z","iopub.execute_input":"2022-04-10T09:33:54.822111Z","iopub.status.idle":"2022-04-10T09:33:55.132771Z","shell.execute_reply.started":"2022-04-10T09:33:54.822071Z","shell.execute_reply":"2022-04-10T09:33:55.131864Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### Model 8 - Re-run","metadata":{}},{"cell_type":"code","source":"train_generator = generator_new(train_path, train_doc, batch_size)\nval_generator = generator_new(val_path, val_doc, batch_size)\n\nmodel_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n  verbose = 1, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 0)\ncallbacks_list = [checkpoint, LR]\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \nhistory = model_8.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T09:36:08.220228Z","iopub.status.idle":"2022-04-10T09:36:08.222427Z","shell.execute_reply.started":"2022-04-10T09:36:08.222205Z","shell.execute_reply":"2022-04-10T09:36:08.222231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}